# Neural network

Neural network是由neural按照不同连接方式组成的网络结构，包括：DNN，CNN，RNN，双向RNN，LSTM等。

神经网络是神经元构成的图（此图是指graph，而不是指image）。神经网络是神经元互相连接构成的一个非循环的图，也就是说一些神经元的输出会作为其他神经元的输入，另外环路是不允许的因为这会使得神经网络的前向传播陷入无止尽的循环中。

当然，神经元之间的排列是有规律的，通常情况下被构建成层层连接的形式，每一层中又有多个神经元。比如说常见的一种层叫做全连接层(fully-connected layer)，表示的是相邻两层之间的神经元两两连接，同层的神经元则互不连接。下图是两个全连接的例子：

![](https://github.com/bobkentt/deep-learning-note/blob/master/pic/20160305203603697.jpg)

命名习惯。注意我们平时说N层神经网络，是不把输入层计算在内的。也就是说一个单层神经网络表示的是输入层接输出层，没有隐含层的网络结构。所以有时候你可能会注意到逻辑回归或SVM被看成是单层神经网络，或者叫人工神经网络（Artificial Neural Networks，ANN）或多层感知机（Multi-Layer Perceptrons，MLP）。还有许多人并不喜欢“神经网络”这个称呼，容易联想成生物学的神经元什么的，所以倾向于把neurons称为units。

输出层和神经网络其他的层不同，最后的输出层通常情况下没有activation function，这是因为最后一层的输出通常用来表示类别的score（特别是分类），score嘛通常是实值的数。

神经网络的大小。衡量某个神经网络有多大，通常有两种方法，1是神经元的数目，2是参数的数目，相比之下第二种更常用。比如说以上图为例：
- 上图左的网络共包含（4+2=6）个神经元（不包括输入层），参数则有[3*4]+[4*2]=20个weights还有[4+2=6]个bias，也就是总共26个可学习的参数。
- 上图右的网络共包含（4+4+1=9）个神经元，参数则有[3*4]+[4*4]+[4*1]=12+16+4=32个weights还有[4+4+1=9]个bias，也就是总共41个可学习的参数。
实际上，现在所有的卷积网络通常都包含亿级的参数，并且由10-20层网络组成(因此说是deep learning)。
